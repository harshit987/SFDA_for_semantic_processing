{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "approach2_negation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyxZhMsRQLcA"
      },
      "source": [
        "## NLP implementation of paper **Source Hypothesis learning for unsupervised domain adaptation**\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2002.08546v4.pdf\n",
        "\n",
        "Code: https://github.com/tim-learn/SHOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6NZ_4sOQyzI"
      },
      "source": [
        "### Relevant Information about Pretrained Model\n",
        "\n",
        "*   inputs of pretrained model is input_ids and attention_masks, one per sample, as model(input_ids,attention_masks)\n",
        "*   Each input_id and attention mask has feature size of 128 for each sample\n",
        "*   output of the model is tuple of shape 2\n",
        "*   **out[0]**: output for classification with shape 2 for binary classification\n",
        "*   **out[1]**: outputs for hidden layers of Roberta with shape 13 in the order: **word embeddings + 12 hidden layers of encoder**\n",
        "*   **out[1][0]**: word embedding layer; **out[1][12]**: last layer of encoder (before pooling)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py_pNYK6qPu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce603c4b-ef16-47aa-f67f-b2509d86289e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQmTNLWnqqWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8c8030-87e0-418d-f7ba-cb72bdb3813a"
      },
      "source": [
        "# !pip3 install -r 'drive/My Drive/source-free-domain-adaptation/baselines/negation/requirements.txt'\n",
        "# !pip3 install torchviz\n",
        "!pip3 install transformers==3.0.2\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n",
        "%cd 'drive/My Drive/source-free-domain-adaptation'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "1.7.0+cu101\n",
            "True\n",
            "10.1\n",
            "[Errno 2] No such file or directory: 'drive/My Drive/source-free-domain-adaptation'\n",
            "/content/drive/My Drive/source-free-domain-adaptation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLr-n2Tatdfy"
      },
      "source": [
        "import logging, os, argparse\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers.data.processors.utils import InputExample, InputFeatures, DataProcessor\n",
        "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import argparse\n",
        "import os, sys, time\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random, pdb, math, copy\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "labels = [\"-1\", \"1\"]\n",
        "max_length = 128\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7a2U7K_yk-q"
      },
      "source": [
        "SEED = 2020\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZKZ78YEuCDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535ef99b-95cf-4d5a-9d84-ce0be50d290c"
      },
      "source": [
        "model_name = \"tmills/roberta_sfda_sharpseed\"\n",
        "config = AutoConfig.from_pretrained(model_name,output_hidden_states=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, config=config)\n",
        "\n",
        "### importing non trainable pretrained model\n",
        "'''\n",
        "inputs of pretrained model is input_ids and attention_masks as model(input_ids,attention_masks)\n",
        "output of the model is tuple of shape 2\n",
        "out[0]: output for classification with shape: 2 for binary classification\n",
        "out[1]: outputs for hidden layers of Roberta with shape 13 in the order: word embeddings + 12 hidden layers of encoder\n",
        "'''\n",
        "fixed_source_net = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "### loading target traiable network\n",
        "target_net = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at tmills/roberta_sfda_sharpseed were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at tmills/roberta_sfda_sharpseed were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghraLjXEmFcd"
      },
      "source": [
        "class NegationDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        self.label_list = [\"-1\", \"1\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, i) -> InputFeatures:\n",
        "        return self.features[i]\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.label_list\n",
        "\n",
        "    @classmethod\n",
        "    def from_tsv(cls, tsv_file, tokenizer):\n",
        "        \"\"\"Creates examples for the test set.\"\"\"\n",
        "        lines = DataProcessor._read_tsv(tsv_file)\n",
        "\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = 'instance-%d' % i\n",
        "            if line[0] in labels: text_a = '\\t'.join(line[1:])\n",
        "            else: text_a = '\\t'.join(line)\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=None))\n",
        "\n",
        "        features = glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, label_list=labels,\n",
        "            output_mode='classification')\n",
        "        return cls(features)\n",
        "\n",
        "# generating train dataset for negation\n",
        "train_dataset = NegationDataset.from_tsv('practice_text/negation/train.tsv', tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZG8n0yqmp7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e0ee1e-ad05-4ea9-8aaf-a775450e354f"
      },
      "source": [
        "### inputs of pretrained model is input_ids and attention_masks as model(input_ids,attention_masks)\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for feat in train_dataset.features:\n",
        "  input_ids.append(feat.input_ids)\n",
        "  attention_masks.append(feat.attention_mask)\n",
        "\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "print(input_ids.shape,attention_masks.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2886, 128]) torch.Size([2886, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYtD0A12nB1R"
      },
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks,torch.arange(input_ids.size(0)))\n",
        "batch_size=32 # 32\n",
        "\n",
        "train_pseudolabels_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            # sampler = RandomSampler(dataset), # Select batches randomly\n",
        "            # shuffle= True,\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            # sampler = RandomSampler(dataset), # Select batches randomly\n",
        "            shuffle= True,\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Rkj1Q4EMXZ"
      },
      "source": [
        "# just for testing\n",
        "# target_net = target_net.cuda()\n",
        "# for i,d in enumerate(train_pseudolabels_dataloader):\n",
        "#     print(d[0].size())\n",
        "#     out = target_net(d[0].cuda(),d[1].cuda())\n",
        "#     # print(np.shape(out[1][-1]))\n",
        "#     # feat = target_net.roberta.pooler(out[1][-1])\n",
        "#     # print(np.shape(feat))\n",
        "#     print('done')\n",
        "#     break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLKmB76Dq-2o"
      },
      "source": [
        "def op_copy(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr0'] = param_group['lr']\n",
        "    return optimizer\n",
        "\n",
        "def lr_scheduler(optimizer, iter_num, max_iter, gamma=10, power=0.75):\n",
        "    decay = (1 + gamma * iter_num / max_iter) ** (-power)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr0'] * decay\n",
        "        param_group['weight_decay'] = 1e-3\n",
        "        param_group['momentum'] = 0.9\n",
        "        param_group['nesterov'] = True\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i_7VnFCGPeY"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "\n",
        "def Entropy(input_):\n",
        "    bs = input_.size(0)\n",
        "    entropy = -input_ * torch.log(input_ + 1e-5)\n",
        "    entropy = torch.sum(entropy, dim=1)\n",
        "    return entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBNISrGHzHuP"
      },
      "source": [
        "def obtain_pseudolabels(loader, target_net): # target_net must be gpu enabled i.e. target_net.cuda()\n",
        "    start_test = True\n",
        "    with torch.no_grad():\n",
        "        iter_test = iter(loader)\n",
        "        for _ in range(len(loader)):\n",
        "            data = iter_test.next()\n",
        "            input_id = data[0].cuda()\n",
        "            attention_mask = data[1].cuda()\n",
        "            out = target_net(input_id,attention_mask)\n",
        "            feas = out[1][-1][:,0,:]\n",
        "            outputs = out[0] \n",
        "            if start_test:\n",
        "                all_fea = feas.float().cpu()\n",
        "                all_output = outputs.float().cpu()\n",
        "                start_test = False\n",
        "            else:\n",
        "                all_fea = torch.cat((all_fea, feas.float().cpu()), 0)\n",
        "                all_output = torch.cat((all_output, outputs.float().cpu()), 0)\n",
        "            # break\n",
        "    all_output = nn.Softmax(dim=1)(all_output)\n",
        "    _, predict = torch.max(all_output, 1)\n",
        "    \n",
        "    all_fea = torch.cat((all_fea, torch.ones(all_fea.size(0), 1)), 1)\n",
        "    all_fea = (all_fea.t() / torch.norm(all_fea, p=2, dim=1)).t()\n",
        "    all_fea = all_fea.float().cpu().detach().numpy()\n",
        "\n",
        "    K = all_output.size(1)\n",
        "    aff = all_output.float().cpu().detach().numpy()\n",
        "    initc = aff.transpose().dot(all_fea)\n",
        "    initc = initc / (1e-8 + aff.sum(axis=0)[:,None])\n",
        "    dd = cdist(all_fea, initc, 'cosine')\n",
        "    pred_label = dd.argmin(axis=1)\n",
        "    \n",
        "    for round in range(1):\n",
        "        aff = np.eye(K)[pred_label]\n",
        "        initc = aff.transpose().dot(all_fea)\n",
        "        initc = initc / (1e-8 + aff.sum(axis=0)[:,None])\n",
        "        dd = cdist(all_fea, initc, 'cosine')\n",
        "        pred_label = dd.argmin(axis=1)\n",
        "\n",
        "    return pred_label.astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yDbpsR-RUz-"
      },
      "source": [
        "# iter_test = iter(train_pseudolabels_dataloader)\n",
        "# for i in range(len(train_pseudolabels_dataloader)):\n",
        "#     data = iter_test.next()\n",
        "#     x = data[0].cuda()\n",
        "#     y = x.cuda()\n",
        "#     print(type(y))\n",
        "#     break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqAqFZiZXrkL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6b7ea69-5e3a-4000-9f49-9dee27077d4e"
      },
      "source": [
        "### test obtain_pseudolabels\n",
        "'''\n",
        "takes about 19 mins for obtaining pseudo labels in CPU\n",
        "Main reason - feed forward is slow\n",
        "- much Faster on GPU\n",
        "'''\n",
        "# import time\n",
        "# s = time.time()\n",
        "# pseudo_labels= obtain_pseudolabels(train_pseudolabels_dataloader,target_net.cuda())\n",
        "# e = time.time()\n",
        "# print(np.shape(pseudo_labels))\n",
        "# print((e-s)/60,'mins')\n",
        "# print(np.shape(pseudo_labels),np.shape(predict))\n",
        "# print(np.sum(pseudo_labels!=predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntakes about 19 mins for obtaining pseudo labels in CPU\\nMain reason - feed forward is slow\\n- much Faster on GPU\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s70Esa_FmDlj"
      },
      "source": [
        "def train_target(loader, target_net, cls_par=0.1, epochs=10, lr=0.01, ent=True, gent=True, ent_par=1.0, \n",
        "                 save=True, save_path='tmp/task1_app2'):\n",
        "    # target_net = target_net.cuda()\n",
        "    target_net.zero_grad()\n",
        "\n",
        "    for i,[name,param] in enumerate(target_net.named_parameters()):\n",
        "        if i>198: #or i<165: ### i>196: pooler+classifier; i<133: first 8 layers \n",
        "            param.requires_grad=False\n",
        "        else:\n",
        "            param.requires_grad=True\n",
        "\n",
        "    param_group = []\n",
        "    for name, param in target_net.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            param_group += [{'params': param, 'lr': lr}]\n",
        "\n",
        "    optimizer = optim.SGD(param_group)\n",
        "    optimizer = op_copy(optimizer)\n",
        "\n",
        "    epoch_loss=0.0\n",
        "    max_iter = epochs * len(loader)\n",
        "    interval_iter = len(loader)\n",
        "    iter_num = 0\n",
        "\n",
        "    while iter_num < max_iter:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        print(iter_num // interval_iter,iter_num % interval_iter,'completed')\n",
        "        if iter_num % interval_iter == 0 and cls_par > 0.001:\n",
        "            target_net.eval()\n",
        "            mem_label = obtain_pseudolabels(train_pseudolabels_dataloader, target_net)\n",
        "            mem_label = torch.from_numpy(mem_label)\n",
        "            mem_label = mem_label.cuda()\n",
        "            target_net.train()\n",
        "\n",
        "        iter_num += 1\n",
        "        lr_scheduler(optimizer, iter_num=iter_num, max_iter=max_iter)\n",
        "\n",
        "        try:\n",
        "            input_id, attention_mask, tar_idx = iter_test.next()\n",
        "        except:\n",
        "            iter_test = iter(loader)\n",
        "            input_id, attention_mask, tar_idx = iter_test.next()\n",
        "\n",
        "        # if input_id.size(0) == 1:\n",
        "        #     continue\n",
        "\n",
        "        input_id = input_id.cuda()\n",
        "        attention_mask = attention_mask.cuda()\n",
        "        target_net.train()\n",
        "        out = target_net(input_id,attention_mask)\n",
        "        features_test = out[1][-1][:,0,:]\n",
        "        outputs_test = out[0]\n",
        "\n",
        "        if cls_par > 0.001:\n",
        "            pred = mem_label[tar_idx]\n",
        "            classifier_loss = cls_par * nn.CrossEntropyLoss()(outputs_test, pred)\n",
        "        else:\n",
        "            classifier_loss = torch.tensor(0.0)\n",
        "            classifier_loss = classifier_loss.cuda()\n",
        "\n",
        "        if ent:\n",
        "            softmax_out = nn.Softmax(dim=1)(outputs_test)\n",
        "            entropy_loss = torch.mean(Entropy(softmax_out))\n",
        "            if gent:\n",
        "                msoftmax = softmax_out.mean(dim=0)\n",
        "                ### torch.sum(-msoftmax * torch.log(msoftmax + 1e-5)) is actually -Ldiv because of (-msoftmax)\n",
        "                ### IM Loss = Lent+Ldiv = Lent-(-Ldiv); that's why substraction\n",
        "                entropy_loss -= torch.sum(-msoftmax * torch.log(msoftmax + 1e-6))\n",
        "\n",
        "            im_loss = entropy_loss * ent_par\n",
        "            classifier_loss += im_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        classifier_loss.backward()\n",
        "        optimizer.step()\n",
        "        # print(target_net.classifier.out_proj.bias.grad)\n",
        "        \n",
        "        epoch_loss += (outputs_test.size(0)*classifier_loss.item())\n",
        "\n",
        "        if iter_num % interval_iter==0:\n",
        "            print('epoch',iter_num//interval_iter,'loss',epoch_loss/2886)\n",
        "            epoch_loss=0.0\n",
        "            # if (iter_num//interval_iter)%5==0:\n",
        "            target_net.eval()\n",
        "            fixed_source_net.eval()\n",
        "            evaluation_in_train(test_dataloader,target_net.cuda(),fixed_source_net.cuda())\n",
        "            \n",
        "\n",
        "        # if save and (iter_num%interval_iter)==0:\n",
        "        #     torch.save(target_net.state_dict(), osp.join(save_path, \"target_net_\" + '0' + \".pt\"))\n",
        "        #     print(\"model saved after epoch\",iter_num//interval_iter)\n",
        "        \n",
        "    return target_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLvcUV-ELkG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e727c3b8-cbb2-4abd-df58-018955e05d44"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "start=time.time()\n",
        "target_net = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "target_net = train_target(train_dataloader,target_net.cuda(),\n",
        "                          lr=1e-3,cls_par=1.0,ent_par=1.0,epochs=20,ent=False,gent=False)\n",
        "end=time.time()\n",
        "\n",
        "print('total time:',(end-start)/60,'mins')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at tmills/roberta_sfda_sharpseed were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0 completed\n",
            "0 1 completed\n",
            "0 2 completed\n",
            "0 3 completed\n",
            "0 4 completed\n",
            "0 5 completed\n",
            "0 6 completed\n",
            "0 7 completed\n",
            "0 8 completed\n",
            "0 9 completed\n",
            "0 10 completed\n",
            "0 11 completed\n",
            "0 12 completed\n",
            "0 13 completed\n",
            "0 14 completed\n",
            "0 15 completed\n",
            "0 16 completed\n",
            "0 17 completed\n",
            "0 18 completed\n",
            "0 19 completed\n",
            "0 20 completed\n",
            "0 21 completed\n",
            "0 22 completed\n",
            "0 23 completed\n",
            "0 24 completed\n",
            "0 25 completed\n",
            "0 26 completed\n",
            "0 27 completed\n",
            "0 28 completed\n",
            "0 29 completed\n",
            "0 30 completed\n",
            "0 31 completed\n",
            "0 32 completed\n",
            "0 33 completed\n",
            "0 34 completed\n",
            "0 35 completed\n",
            "0 36 completed\n",
            "0 37 completed\n",
            "0 38 completed\n",
            "0 39 completed\n",
            "0 40 completed\n",
            "0 41 completed\n",
            "0 42 completed\n",
            "0 43 completed\n",
            "0 44 completed\n",
            "0 45 completed\n",
            "0 46 completed\n",
            "0 47 completed\n",
            "0 48 completed\n",
            "0 49 completed\n",
            "0 50 completed\n",
            "0 51 completed\n",
            "0 52 completed\n",
            "0 53 completed\n",
            "0 54 completed\n",
            "0 55 completed\n",
            "0 56 completed\n",
            "0 57 completed\n",
            "0 58 completed\n",
            "0 59 completed\n",
            "0 60 completed\n",
            "0 61 completed\n",
            "0 62 completed\n",
            "0 63 completed\n",
            "0 64 completed\n",
            "0 65 completed\n",
            "0 66 completed\n",
            "0 67 completed\n",
            "0 68 completed\n",
            "0 69 completed\n",
            "0 70 completed\n",
            "0 71 completed\n",
            "0 72 completed\n",
            "0 73 completed\n",
            "0 74 completed\n",
            "0 75 completed\n",
            "0 76 completed\n",
            "0 77 completed\n",
            "0 78 completed\n",
            "0 79 completed\n",
            "0 80 completed\n",
            "0 81 completed\n",
            "0 82 completed\n",
            "0 83 completed\n",
            "0 84 completed\n",
            "0 85 completed\n",
            "0 86 completed\n",
            "0 87 completed\n",
            "0 88 completed\n",
            "0 89 completed\n",
            "0 90 completed\n",
            "epoch 1 loss 0.0976351496407908\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.828229     0.819859  0.836771\n",
            "1 0 completed\n",
            "1 1 completed\n",
            "1 2 completed\n",
            "1 3 completed\n",
            "1 4 completed\n",
            "1 5 completed\n",
            "1 6 completed\n",
            "1 7 completed\n",
            "1 8 completed\n",
            "1 9 completed\n",
            "1 10 completed\n",
            "1 11 completed\n",
            "1 12 completed\n",
            "1 13 completed\n",
            "1 14 completed\n",
            "1 15 completed\n",
            "1 16 completed\n",
            "1 17 completed\n",
            "1 18 completed\n",
            "1 19 completed\n",
            "1 20 completed\n",
            "1 21 completed\n",
            "1 22 completed\n",
            "1 23 completed\n",
            "1 24 completed\n",
            "1 25 completed\n",
            "1 26 completed\n",
            "1 27 completed\n",
            "1 28 completed\n",
            "1 29 completed\n",
            "1 30 completed\n",
            "1 31 completed\n",
            "1 32 completed\n",
            "1 33 completed\n",
            "1 34 completed\n",
            "1 35 completed\n",
            "1 36 completed\n",
            "1 37 completed\n",
            "1 38 completed\n",
            "1 39 completed\n",
            "1 40 completed\n",
            "1 41 completed\n",
            "1 42 completed\n",
            "1 43 completed\n",
            "1 44 completed\n",
            "1 45 completed\n",
            "1 46 completed\n",
            "1 47 completed\n",
            "1 48 completed\n",
            "1 49 completed\n",
            "1 50 completed\n",
            "1 51 completed\n",
            "1 52 completed\n",
            "1 53 completed\n",
            "1 54 completed\n",
            "1 55 completed\n",
            "1 56 completed\n",
            "1 57 completed\n",
            "1 58 completed\n",
            "1 59 completed\n",
            "1 60 completed\n",
            "1 61 completed\n",
            "1 62 completed\n",
            "1 63 completed\n",
            "1 64 completed\n",
            "1 65 completed\n",
            "1 66 completed\n",
            "1 67 completed\n",
            "1 68 completed\n",
            "1 69 completed\n",
            "1 70 completed\n",
            "1 71 completed\n",
            "1 72 completed\n",
            "1 73 completed\n",
            "1 74 completed\n",
            "1 75 completed\n",
            "1 76 completed\n",
            "1 77 completed\n",
            "1 78 completed\n",
            "1 79 completed\n",
            "1 80 completed\n",
            "1 81 completed\n",
            "1 82 completed\n",
            "1 83 completed\n",
            "1 84 completed\n",
            "1 85 completed\n",
            "1 86 completed\n",
            "1 87 completed\n",
            "1 88 completed\n",
            "1 89 completed\n",
            "1 90 completed\n",
            "epoch 2 loss 0.06509350648127904\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.772895     0.712879  0.843946\n",
            "2 0 completed\n",
            "2 1 completed\n",
            "2 2 completed\n",
            "2 3 completed\n",
            "2 4 completed\n",
            "2 5 completed\n",
            "2 6 completed\n",
            "2 7 completed\n",
            "2 8 completed\n",
            "2 9 completed\n",
            "2 10 completed\n",
            "2 11 completed\n",
            "2 12 completed\n",
            "2 13 completed\n",
            "2 14 completed\n",
            "2 15 completed\n",
            "2 16 completed\n",
            "2 17 completed\n",
            "2 18 completed\n",
            "2 19 completed\n",
            "2 20 completed\n",
            "2 21 completed\n",
            "2 22 completed\n",
            "2 23 completed\n",
            "2 24 completed\n",
            "2 25 completed\n",
            "2 26 completed\n",
            "2 27 completed\n",
            "2 28 completed\n",
            "2 29 completed\n",
            "2 30 completed\n",
            "2 31 completed\n",
            "2 32 completed\n",
            "2 33 completed\n",
            "2 34 completed\n",
            "2 35 completed\n",
            "2 36 completed\n",
            "2 37 completed\n",
            "2 38 completed\n",
            "2 39 completed\n",
            "2 40 completed\n",
            "2 41 completed\n",
            "2 42 completed\n",
            "2 43 completed\n",
            "2 44 completed\n",
            "2 45 completed\n",
            "2 46 completed\n",
            "2 47 completed\n",
            "2 48 completed\n",
            "2 49 completed\n",
            "2 50 completed\n",
            "2 51 completed\n",
            "2 52 completed\n",
            "2 53 completed\n",
            "2 54 completed\n",
            "2 55 completed\n",
            "2 56 completed\n",
            "2 57 completed\n",
            "2 58 completed\n",
            "2 59 completed\n",
            "2 60 completed\n",
            "2 61 completed\n",
            "2 62 completed\n",
            "2 63 completed\n",
            "2 64 completed\n",
            "2 65 completed\n",
            "2 66 completed\n",
            "2 67 completed\n",
            "2 68 completed\n",
            "2 69 completed\n",
            "2 70 completed\n",
            "2 71 completed\n",
            "2 72 completed\n",
            "2 73 completed\n",
            "2 74 completed\n",
            "2 75 completed\n",
            "2 76 completed\n",
            "2 77 completed\n",
            "2 78 completed\n",
            "2 79 completed\n",
            "2 80 completed\n",
            "2 81 completed\n",
            "2 82 completed\n",
            "2 83 completed\n",
            "2 84 completed\n",
            "2 85 completed\n",
            "2 86 completed\n",
            "2 87 completed\n",
            "2 88 completed\n",
            "2 89 completed\n",
            "2 90 completed\n",
            "epoch 3 loss 0.07703259135180167\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.743259     0.658587  0.852915\n",
            "3 0 completed\n",
            "3 1 completed\n",
            "3 2 completed\n",
            "3 3 completed\n",
            "3 4 completed\n",
            "3 5 completed\n",
            "3 6 completed\n",
            "3 7 completed\n",
            "3 8 completed\n",
            "3 9 completed\n",
            "3 10 completed\n",
            "3 11 completed\n",
            "3 12 completed\n",
            "3 13 completed\n",
            "3 14 completed\n",
            "3 15 completed\n",
            "3 16 completed\n",
            "3 17 completed\n",
            "3 18 completed\n",
            "3 19 completed\n",
            "3 20 completed\n",
            "3 21 completed\n",
            "3 22 completed\n",
            "3 23 completed\n",
            "3 24 completed\n",
            "3 25 completed\n",
            "3 26 completed\n",
            "3 27 completed\n",
            "3 28 completed\n",
            "3 29 completed\n",
            "3 30 completed\n",
            "3 31 completed\n",
            "3 32 completed\n",
            "3 33 completed\n",
            "3 34 completed\n",
            "3 35 completed\n",
            "3 36 completed\n",
            "3 37 completed\n",
            "3 38 completed\n",
            "3 39 completed\n",
            "3 40 completed\n",
            "3 41 completed\n",
            "3 42 completed\n",
            "3 43 completed\n",
            "3 44 completed\n",
            "3 45 completed\n",
            "3 46 completed\n",
            "3 47 completed\n",
            "3 48 completed\n",
            "3 49 completed\n",
            "3 50 completed\n",
            "3 51 completed\n",
            "3 52 completed\n",
            "3 53 completed\n",
            "3 54 completed\n",
            "3 55 completed\n",
            "3 56 completed\n",
            "3 57 completed\n",
            "3 58 completed\n",
            "3 59 completed\n",
            "3 60 completed\n",
            "3 61 completed\n",
            "3 62 completed\n",
            "3 63 completed\n",
            "3 64 completed\n",
            "3 65 completed\n",
            "3 66 completed\n",
            "3 67 completed\n",
            "3 68 completed\n",
            "3 69 completed\n",
            "3 70 completed\n",
            "3 71 completed\n",
            "3 72 completed\n",
            "3 73 completed\n",
            "3 74 completed\n",
            "3 75 completed\n",
            "3 76 completed\n",
            "3 77 completed\n",
            "3 78 completed\n",
            "3 79 completed\n",
            "3 80 completed\n",
            "3 81 completed\n",
            "3 82 completed\n",
            "3 83 completed\n",
            "3 84 completed\n",
            "3 85 completed\n",
            "3 86 completed\n",
            "3 87 completed\n",
            "3 88 completed\n",
            "3 89 completed\n",
            "3 90 completed\n",
            "epoch 4 loss 0.06837030120093275\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.722983     0.627892  0.852018\n",
            "4 0 completed\n",
            "4 1 completed\n",
            "4 2 completed\n",
            "4 3 completed\n",
            "4 4 completed\n",
            "4 5 completed\n",
            "4 6 completed\n",
            "4 7 completed\n",
            "4 8 completed\n",
            "4 9 completed\n",
            "4 10 completed\n",
            "4 11 completed\n",
            "4 12 completed\n",
            "4 13 completed\n",
            "4 14 completed\n",
            "4 15 completed\n",
            "4 16 completed\n",
            "4 17 completed\n",
            "4 18 completed\n",
            "4 19 completed\n",
            "4 20 completed\n",
            "4 21 completed\n",
            "4 22 completed\n",
            "4 23 completed\n",
            "4 24 completed\n",
            "4 25 completed\n",
            "4 26 completed\n",
            "4 27 completed\n",
            "4 28 completed\n",
            "4 29 completed\n",
            "4 30 completed\n",
            "4 31 completed\n",
            "4 32 completed\n",
            "4 33 completed\n",
            "4 34 completed\n",
            "4 35 completed\n",
            "4 36 completed\n",
            "4 37 completed\n",
            "4 38 completed\n",
            "4 39 completed\n",
            "4 40 completed\n",
            "4 41 completed\n",
            "4 42 completed\n",
            "4 43 completed\n",
            "4 44 completed\n",
            "4 45 completed\n",
            "4 46 completed\n",
            "4 47 completed\n",
            "4 48 completed\n",
            "4 49 completed\n",
            "4 50 completed\n",
            "4 51 completed\n",
            "4 52 completed\n",
            "4 53 completed\n",
            "4 54 completed\n",
            "4 55 completed\n",
            "4 56 completed\n",
            "4 57 completed\n",
            "4 58 completed\n",
            "4 59 completed\n",
            "4 60 completed\n",
            "4 61 completed\n",
            "4 62 completed\n",
            "4 63 completed\n",
            "4 64 completed\n",
            "4 65 completed\n",
            "4 66 completed\n",
            "4 67 completed\n",
            "4 68 completed\n",
            "4 69 completed\n",
            "4 70 completed\n",
            "4 71 completed\n",
            "4 72 completed\n",
            "4 73 completed\n",
            "4 74 completed\n",
            "4 75 completed\n",
            "4 76 completed\n",
            "4 77 completed\n",
            "4 78 completed\n",
            "4 79 completed\n",
            "4 80 completed\n",
            "4 81 completed\n",
            "4 82 completed\n",
            "4 83 completed\n",
            "4 84 completed\n",
            "4 85 completed\n",
            "4 86 completed\n",
            "4 87 completed\n",
            "4 88 completed\n",
            "4 89 completed\n",
            "4 90 completed\n",
            "epoch 5 loss 0.05618942066464297\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.71203      0.612945  0.849327\n",
            "5 0 completed\n",
            "5 1 completed\n",
            "5 2 completed\n",
            "5 3 completed\n",
            "5 4 completed\n",
            "5 5 completed\n",
            "5 6 completed\n",
            "5 7 completed\n",
            "5 8 completed\n",
            "5 9 completed\n",
            "5 10 completed\n",
            "5 11 completed\n",
            "5 12 completed\n",
            "5 13 completed\n",
            "5 14 completed\n",
            "5 15 completed\n",
            "5 16 completed\n",
            "5 17 completed\n",
            "5 18 completed\n",
            "5 19 completed\n",
            "5 20 completed\n",
            "5 21 completed\n",
            "5 22 completed\n",
            "5 23 completed\n",
            "5 24 completed\n",
            "5 25 completed\n",
            "5 26 completed\n",
            "5 27 completed\n",
            "5 28 completed\n",
            "5 29 completed\n",
            "5 30 completed\n",
            "5 31 completed\n",
            "5 32 completed\n",
            "5 33 completed\n",
            "5 34 completed\n",
            "5 35 completed\n",
            "5 36 completed\n",
            "5 37 completed\n",
            "5 38 completed\n",
            "5 39 completed\n",
            "5 40 completed\n",
            "5 41 completed\n",
            "5 42 completed\n",
            "5 43 completed\n",
            "5 44 completed\n",
            "5 45 completed\n",
            "5 46 completed\n",
            "5 47 completed\n",
            "5 48 completed\n",
            "5 49 completed\n",
            "5 50 completed\n",
            "5 51 completed\n",
            "5 52 completed\n",
            "5 53 completed\n",
            "5 54 completed\n",
            "5 55 completed\n",
            "5 56 completed\n",
            "5 57 completed\n",
            "5 58 completed\n",
            "5 59 completed\n",
            "5 60 completed\n",
            "5 61 completed\n",
            "5 62 completed\n",
            "5 63 completed\n",
            "5 64 completed\n",
            "5 65 completed\n",
            "5 66 completed\n",
            "5 67 completed\n",
            "5 68 completed\n",
            "5 69 completed\n",
            "5 70 completed\n",
            "5 71 completed\n",
            "5 72 completed\n",
            "5 73 completed\n",
            "5 74 completed\n",
            "5 75 completed\n",
            "5 76 completed\n",
            "5 77 completed\n",
            "5 78 completed\n",
            "5 79 completed\n",
            "5 80 completed\n",
            "5 81 completed\n",
            "5 82 completed\n",
            "5 83 completed\n",
            "5 84 completed\n",
            "5 85 completed\n",
            "5 86 completed\n",
            "5 87 completed\n",
            "5 88 completed\n",
            "5 89 completed\n",
            "5 90 completed\n",
            "epoch 6 loss 0.06526173737573897\n",
            "model         f1 score    precision    recall\n",
            "----------  ----------  -----------  --------\n",
            "pretrained    0.834019     0.850746  0.817937\n",
            "trained       0.700637     0.601673  0.838565\n",
            "6 0 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JVYQyTMU7Nc"
      },
      "source": [
        "def load_testdata():\n",
        "    test_dataset = NegationDataset.from_tsv('practice_text/negation/dev.tsv', tokenizer)\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for feat in test_dataset.features:\n",
        "        input_ids.append(feat.input_ids)\n",
        "        attention_masks.append(feat.attention_mask)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    print(input_ids.shape,attention_masks.shape)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, attention_masks,torch.arange(input_ids.size(0)))\n",
        "    \n",
        "    # batch_size=16 # 32\n",
        "    print(batch_size)\n",
        "    test_dataloader = DataLoader(dataset, batch_size = batch_size)\n",
        "\n",
        "    return test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-gsBZjPU_Le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ebbdc4-02b2-44f7-fc8e-2f7fd38b5995"
      },
      "source": [
        "test_dataloader = load_testdata()\n",
        "def evaluation_in_train(test_dataloader,target_net,fixed_source_net):\n",
        "    # target_net.load_state_dict(torch.load('tmp/task1_app2/target_net_0.pt'))\n",
        "    target_net.eval()\n",
        "    start=True\n",
        "    for data in test_dataloader:\n",
        "        out = target_net(data[0].cuda(),data[1].cuda())\n",
        "        if start:\n",
        "            predict = out[0].cpu().detach().numpy()\n",
        "            start = False\n",
        "        else:\n",
        "            predict = np.concatenate((predict,out[0].cpu().detach().numpy()))\n",
        "    predict = np.argmax(predict,1)\n",
        "    # print(predict.shape)\n",
        "\n",
        "    # fixed_source_net = fixed_source_net.cuda()\n",
        "    # start=True\n",
        "    # for data in test_dataloader:\n",
        "    #     out1 = fixed_source_net(data[0].cuda(),data[1].cuda())\n",
        "    #     if start:\n",
        "    #         predict1 = out1[0].cpu().detach().numpy()\n",
        "    #         start = False\n",
        "    #     else:\n",
        "    #         predict1 = np.concatenate((predict1,out1[0].cpu().detach().numpy()))\n",
        "    # predict1 = np.argmax(predict1,1)\n",
        "    # print(predict1.shape)\n",
        "\n",
        "    pred = np.array([0]*5545)\n",
        "    for i in range(5545):\n",
        "        pred[i] = labels[predict[i]]\n",
        "    # print(pred.shape)\n",
        "\n",
        "    # pred1 = np.array([0]*5545)\n",
        "    # for i in range(5545):\n",
        "    #     pred1[i] = labels[predict1[i]]\n",
        "    # print(pred1.shape)\n",
        "    \n",
        "    test_true = np.loadtxt('practice_text/negation/dev_labels.txt',dtype=np.int32)\n",
        "\n",
        "    # print('pretrained',f1_score(test_true,pred1),precision_score(test_true,pred1),recall_score(test_true,pred1))\n",
        "    # print('trained',f1_score(test_true,pred),precision_score(test_true,pred),recall_score(test_true,pred))\n",
        "\n",
        "    scores = [['pretrained',0.834019,0.850746,0.817937],\n",
        "              ['trained',f1_score(test_true,pred), precision_score(test_true,pred), recall_score(test_true,pred)]]\n",
        "    print(tabulate(scores,headers=['model','f1 score','precision','recall']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5545, 128]) torch.Size([5545, 128])\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKcqDDuJRZi5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "04096ed2-fc68-4487-8018-dddddafb36a2"
      },
      "source": [
        "evaluation(test_dataloader,target_net,fixed_source_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c94f8dfb44cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_source_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JusfUAsUhAO"
      },
      "source": [
        "for i,[name,param] in enumerate(target_net.named_parameters()):\n",
        "    print(i,name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}